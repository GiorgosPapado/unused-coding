{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': '<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7FB9FDBD2890>', 'caption': 'The bowl is bellow the bowl. The bowl is above the bowl. The bowl is in front of the bowl. The bowl is behind the bowl. The broccoli is bellow the bowl. The bowl is above the broccoli. The broccoli is in front of the bowl. The bowl is behind...', 'image_path': 'resized_train/COCO_train2014_000000000009.jpg', 'height': 224, 'width': 224, 'image_id': 0, 'file_name': 'COCO_train2014_000000000009'}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"sadassa17/rgb-spatial-dataset\", split='train', streaming=True)\n",
    "print(next(iter(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 0, 'image': '<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7FB9E7D50730>', 'caption': 'The dog is to the right of the sports ball. The sports ball is to the left of the dog.', 'image_path': 'resized_val/COCO_val2014_000000000042.jpg', 'height': 224, 'width': 224, 'image_id': 0, 'file_name': 'COCO_val2014_000000000042'}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "val_dataset = load_dataset(\"sadassa17/rgb-spatial-dataset\", split='validation', streaming=True)\n",
    "print(next(iter(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.iterable_dataset.IterableDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####CODE TO CREATE A LIST OF TENSORS FOR THE WHOLE TRAINING DATASET####\n",
    "import torch\n",
    "loaded_tensor1 = torch.load('f16train1_tensor_data.pt')\n",
    "loaded_tensor2 = torch.load('f16train2_tensor_data.pt')\n",
    "loaded_tensor3 = torch.load('f16train3_tensor_data.pt')\n",
    "loaded_tensor4 = torch.load('full_training_tensor.pt')\n",
    "chunk_size = 1000  # Adjust the chunk size based on available memory\n",
    "num_chunks = len(loaded_tensor1) // chunk_size\n",
    "chunk_tensors = []  # List to store individual chunks\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size\n",
    "    chunk_tensor = torch.cat((loaded_tensor1[start_idx:end_idx], loaded_tensor2[start_idx:end_idx], loaded_tensor3[start_idx:end_idx]), dim=0)\n",
    "    chunk_tensors.append(chunk_tensor)  # Store the chunk_tensor in the list for later use\n",
    "\n",
    "\n",
    "# If there are remaining elements, create a final chunk for them\n",
    "if num_chunks * chunk_size < len(loaded_tensor1):\n",
    "    remaining_tensor = torch.cat((loaded_tensor1[num_chunks * chunk_size:], \n",
    "                                 loaded_tensor2[num_chunks * chunk_size:], \n",
    "                                 loaded_tensor3[num_chunks * chunk_size:]), dim=0)\n",
    "    chunk_tensors.append(remaining_tensor)\n",
    "\n",
    "torch.save(chunk_tensors, 'tensor_list.pth')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CODE TO SAVE THE TENSOR AFTER CONVERTING FROM A LIST OF TENSOR TO ONE COMPLETE TENSOR \n",
    "# tensor = torch.cat(chunk_tensors, dim=0)\n",
    "# tensor.size()\n",
    "# file_name = 'full_training_tensor.pt'\n",
    "# torch.save(tensor, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([82611, 1, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tensor = torch.cat((loaded_tensor1, loaded_tensor2), dim=0)\n",
    "file_name = f'{full_tensor}_tensor_data.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaded_tensor3 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mf16train3_tensor_data.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "loaded_tensor3 = torch.load('train3_tensor_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tensor = torch.cat((loaded_tensor1, loaded_tensor2), dim=0)\n",
    "file_name = f'{full_tensor}_tensor_data.pt'\n",
    "torch.save(full_tensor, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.3.crossattention.c_attn.weight', 'h.7.ln_cross_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.3.ln_cross_attn.bias', 'h.7.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.8.ln_cross_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.2.crossattention.q_attn.bias', 'h.6.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.4.ln_cross_attn.bias', 'h.11.crossattention.c_proj.bias', 'h.7.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.8.ln_cross_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.5.ln_cross_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.8.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.1.ln_cross_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.weight', 'h.2.crossattention.c_attn.weight', 'h.1.ln_cross_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.bias', 'h.5.ln_cross_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.bias', 'h.3.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias', 'h.9.crossattention.c_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.1.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.4.crossattention.c_attn.bias', 'h.2.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.10.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.bias', 'h.0.ln_cross_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.9.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.10.ln_cross_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.6.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.0.ln_cross_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.1.crossattention.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor, AutoImageProcessor\n",
    "\n",
    "image_encoder_model = \"Centaur31/vit-base\"\n",
    "text_decode_model = \"gpt2\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    image_encoder_model, text_decode_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vit-gpt-model/tokenizer_config.json',\n",
       " 'vit-gpt-model/special_tokens_map.json',\n",
       " 'vit-gpt-model/vocab.json',\n",
       " 'vit-gpt-model/merges.txt',\n",
       " 'vit-gpt-model/added_tokens.json',\n",
       " 'vit-gpt-model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image feature extractor\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(image_encoder_model)\n",
    "# text tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_decode_model)\n",
    "\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.encoder.embeddings.patch_embeddings.projection\n",
    "\n",
    "output_dir = \"vit-gpt-model\"\n",
    "model.save_pretrained(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_items_in_folder(folder_path):\n",
    "    items_list = []\n",
    "\n",
    "    # Iterate over each item (files or directories) in the folder\n",
    "    for item in os.listdir(folder_path):\n",
    "        # Get the full path of the item\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        # Append the item's path to the list\n",
    "        items_list.append(item_path)\n",
    "\n",
    "    return items_list\n",
    "\n",
    "# Replace 'folder_path' with the path to the folder you want to read\n",
    "folder_path = 'resized_train'\n",
    "items_list = list_items_in_folder(folder_path)\n",
    "items_list = sorted(items_list)\n",
    "\n",
    "# Replace 'folder_path' with the path to the folder you want to read\n",
    "val_folder_path = 'resized_val'\n",
    "val_items_list = list_items_in_folder(val_folder_path)\n",
    "val_items_list=sorted(val_items_list)\n",
    "\n",
    "###CODING TO RESIZE IMAGE DATASETS BEFORE PROCESSING####\n",
    "# # List to store resized images\n",
    "# resized_images = []\n",
    "# from PIL import Image\n",
    "# save_directory = \"resized_train\"\n",
    "# if not os.path.exists(save_directory):\n",
    "#     os.makedirs(save_directory)\n",
    "# # Resize images and append them to the resized_images list\n",
    "# for image_path in items_list:\n",
    "#     # Open the original image using PIL\n",
    "#     original_image = Image.open(image_path)\n",
    "#     resized_image = original_image.resize((224, 224))\n",
    "#     file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "#     new_file_name = f\"{file_name}.jpg\"\n",
    "#     save_path = os.path.join(save_directory, new_file_name)\n",
    "#     resized_image.save(save_path)\n",
    "#     original_image.close()\n",
    "#     resized_image.close()\n",
    "\n",
    "\n",
    "#from datasets import load_dataset, Image, Dataset\n",
    "from datasets import Image as dataset_Image, Dataset\n",
    "data = {\"image\": items_list}\n",
    "# Step 3: Convert the list to a Dataset object\n",
    "dataset = Dataset.from_dict(data)\n",
    "# Step 4: Cast the \"image\" column to the Image() type\n",
    "dataset = dataset.cast_column(\"image\", dataset_Image())\n",
    "\n",
    "#EVALUATION\n",
    "val_data = {\"image\": val_items_list}\n",
    "#Convert the list to a Dataset object\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "\n",
    "#Cast the \"image\" column to the Image() type\n",
    "val_dataset = val_dataset.cast_column(\"image\", dataset_Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION\n",
    "val_heights_list = []\n",
    "val_widths_list = []\n",
    "val_image_id_counter = 0\n",
    "val_text_id_counter = 0\n",
    "val_image_id_list = []\n",
    "val_text_id_list = []\n",
    "# Loop through the 'image' column of the dataset\n",
    "for image in val_dataset['image']:\n",
    "    # Get the height and width of the current image\n",
    "    val_height, val_width = image.size\n",
    "\n",
    "    # Append the height and width to their respective lists\n",
    "    val_heights_list.append(val_height)\n",
    "    val_widths_list.append(val_width)\n",
    "    val_image_id = val_image_id_counter\n",
    "    val_image_id_list.append(val_image_id)\n",
    "    val_image_id_counter += 1\n",
    "\n",
    "heights_list = []\n",
    "widths_list = []\n",
    "image_id_counter = 0\n",
    "text_id_counter = 0\n",
    "image_id_list = []\n",
    "text_id_list = []\n",
    "#Loop through the 'image' column of the dataset\n",
    "for image in dataset['image']:\n",
    "    # Get the height and width of the current image\n",
    "    height, width = image.size\n",
    "\n",
    "    # Append the height and width to their respective lists\n",
    "    heights_list.append(height)\n",
    "    widths_list.append(width)\n",
    "    image_id = image_id_counter\n",
    "    image_id_list.append(image_id)\n",
    "    image_id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# val_heights_list = []\n",
    "# val_widths_list = []\n",
    "# val_image_id_counter = 0\n",
    "# val_text_id_counter = 0\n",
    "# val_image_id_list = []\n",
    "# val_text_id_list = []\n",
    "\n",
    "# with tqdm(desc=\"Processing validation Images\") as progress_bar:\n",
    "# # Loop through the 'image' column of the dataset\n",
    "#     for image in val_dataset['image']:\n",
    "#         # Get the height and width of the current image\n",
    "#         val_height, val_width = image.size\n",
    "\n",
    "#         # Append the height and width to their respective lists\n",
    "#         val_heights_list.append(val_height)\n",
    "#         val_widths_list.append(val_width)\n",
    "#         val_image_id = val_image_id_counter\n",
    "#         val_image_id_list.append(val_image_id)\n",
    "#         val_image_id_counter += 1\n",
    "\n",
    "#         # Update the tqdm progress bar\n",
    "#         progress_bar.update(1)\n",
    "\n",
    "     \n",
    "\n",
    "# heights_list = []\n",
    "# widths_list = []\n",
    "# image_id_counter = 0\n",
    "# text_id_counter = 0\n",
    "# image_id_list = []\n",
    "# text_id_list = []\n",
    "\n",
    "# progress_bar = tqdm(desc=\"Processing Training Images\")\n",
    "# for image in dataset['image']:\n",
    "#     # Get the height and width of the current image\n",
    "#     height, width = image.size\n",
    "\n",
    "#     # Append the height and width to their respective lists\n",
    "#     heights_list.append(height)\n",
    "#     widths_list.append(width)\n",
    "#     image_id = image_id_counter\n",
    "#     image_id_list.append(image_id)\n",
    "#     image_id_counter += 1\n",
    "\n",
    "#     # Update the tqdm progress bar\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# # Close the tqdm progress bar after the loop is finished\n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_numbers(text_descriptions):\n",
    "    clean_text_descriptions = []\n",
    "    for line in text_descriptions:\n",
    "        clean_text_descriptions.append((re.sub(r'\\d+', '', line)).strip())  # Remove digits and leading/trailing spaces\n",
    "    combined_text = '. '.join(clean_text_descriptions) + '.'  # Add a period at the end\n",
    "    return combined_text\n",
    "\n",
    "def limit_words(text, word_limit=200):\n",
    "    words = text.split()\n",
    "    if len(words) > word_limit:\n",
    "        return ' '.join(words[:word_limit]) + '...'  # Add ellipsis if text is truncated\n",
    "    return text\n",
    "\n",
    "text_files_path = '/home/vcl3d/coco_dataset_VOX/train2014_desc'\n",
    "text_files = [file for file in os.listdir(text_files_path) if file.endswith('_desc.txt')]\n",
    "text_files = sorted(text_files)\n",
    "# Create a dictionary to store text descriptions with image filenames (without extension) as keys\n",
    "text_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text descriptions from each text file and match them with the images\n",
    "for text_file in text_files:\n",
    "    image_name = os.path.splitext(text_file)[0]\n",
    "    text_file_path = os.path.join(text_files_path, text_file)\n",
    "\n",
    "    with open(text_file_path, 'r') as file:\n",
    "        text_descriptions = file.read().splitlines()\n",
    "\n",
    "    text_dict[image_name] = text_descriptions\n",
    "\n",
    "    text_descriptions = remove_numbers(text_descriptions)\n",
    "    text_descriptions = limit_words(text_descriptions , word_limit=50)\n",
    "    text_dict[image_name] = text_descriptions\n",
    "# Convert image paths to strings by extracting the file name from the full path\n",
    "image_filenames = [os.path.splitext(os.path.basename(image_path.filename))[0] for image_path in dataset[\"image\"]]\n",
    "text_filenames = [os.path.splitext(os.path.basename(image_path.filename))[0] + '_desc'  for image_path in dataset[\"image\"]]\n",
    "# Add the \"text\" column to the dimg dataset\n",
    "dataset = dataset.add_column(\"caption\", [text_dict[filename] for filename in text_filenames])\n",
    "dataset = dataset.add_column(\"image_path\", items_list)\n",
    "dataset = dataset.add_column(\"height\", heights_list)\n",
    "dataset = dataset.add_column(\"width\", widths_list)\n",
    "dataset = dataset.add_column(\"image_id\", image_id_list)\n",
    "dataset = dataset.add_column(\"file_name\", image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#EVALUATION\n",
    "val_text_files_path = '/home/vcl3d/coco_dataset_VOX/val2014_desc'\n",
    "val_text_files = [file for file in os.listdir(val_text_files_path) if file.endswith('_desc.txt')]\n",
    "val_text_files = sorted(val_text_files)\n",
    "\n",
    "# Create a dictionary to store text descriptions with image filenames (without extension) as keys\n",
    "val_text_dict = {}\n",
    "\n",
    "# Load text descriptions from each text file and match them with the images\n",
    "for text_file in val_text_files:\n",
    "    val_image_name = os.path.splitext(text_file)[0]\n",
    "    val_text_file_path = os.path.join(val_text_files_path, text_file)\n",
    "\n",
    "    with open(val_text_file_path, 'r') as file:\n",
    "        val_text_descriptions = file.read().splitlines()\n",
    "\n",
    "    val_text_dict[val_image_name] = val_text_descriptions\n",
    "\n",
    "    val_text_descriptions = remove_numbers(val_text_descriptions)\n",
    "    val_text_descriptions = limit_words(val_text_descriptions , word_limit=50)\n",
    "    val_text_dict[val_image_name] = val_text_descriptions\n",
    "\n",
    "# Convert image paths to strings by extracting the file name from the full path\n",
    "val_image_filenames = [os.path.splitext(os.path.basename(val_image_path.filename))[0] for val_image_path in val_dataset[\"image\"]]\n",
    "val_text_filenames = [os.path.splitext(os.path.basename(val_image_path.filename))[0] + '_desc'  for val_image_path in val_dataset[\"image\"]]\n",
    "# Add the \"text\" column to the dimg dataset\n",
    "val_dataset = val_dataset.add_column(\"caption\", [val_text_dict[filename] for filename in val_text_filenames])\n",
    "val_dataset = val_dataset.add_column(\"image_path\", val_items_list)\n",
    "val_dataset = val_dataset.add_column(\"height\", val_heights_list)\n",
    "val_dataset = val_dataset.add_column(\"width\", val_widths_list)\n",
    "val_dataset = val_dataset.add_column(\"image_id\", val_image_id_list)\n",
    "val_dataset = val_dataset.add_column(\"file_name\", val_image_filenames)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###UNCOMMENT TO SAVE TENSORS FROM DATASET\n",
    "# #gtp version\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def feature_extraction_fn(image_paths, tensor_name):\n",
    "# Define your image_paths and feature_extractor\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "\n",
    "    # Define your image_paths and feature_extractor\n",
    "    batch_size = 100 # Choose an appropriate batch size based on your system's memory capacity\n",
    "\n",
    "    num_images = len(image_paths)\n",
    "    num_batches = int(np.ceil(num_images / batch_size))\n",
    "\n",
    "    inputs = []  # List to store processed image data\n",
    "\n",
    "    with tqdm(total=num_batches, desc=\"Processing Batches\") as pbar_batch:\n",
    "        for i in range(0, num_images, batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_images = []  # List to store images for the current batch\n",
    "            \n",
    "            # Process images in the current batch\n",
    "            for image_file in batch_paths:\n",
    "                        with Image.open(image_file) as img:\n",
    "                            image_path = feature_extractor(img, return_tensors=\"np\")\n",
    "                            batch_images.append(image_path.pixel_values)\n",
    "            # Convert the list of batch images to a numpy array\n",
    "            batch_array = np.array(batch_images)\n",
    "            \n",
    "            # Convert the numpy array to a PyTorch tensor\n",
    "            batch_tensor = torch.tensor(batch_array)\n",
    "            \n",
    "            # Append the batch tensor to the inputs list\n",
    "            inputs.append(batch_tensor)\n",
    "            pbar_batch.update(1)\n",
    "\n",
    "    # Concatenate the list of batch tensors along the first dimension to create the final tensor\n",
    "    pbar_batch.close()\n",
    "    tensor = torch.cat(inputs, dim=0)\n",
    "    file_name = f'{tensor_name}_tensor_data.pt'\n",
    "    print(tensor.dtype)\n",
    "    #tensor = torch.quantize_per_tensor(float_tensor, scale=0.2, zero_point=0, dtype=torch.qint32)\n",
    "    tensor = tensor.half()\n",
    "    torch.save(tensor, file_name)\n",
    "    print(tensor.dtype)\n",
    "    print(\"tensor saved in:\", file_name)\n",
    "    return tensor   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_index_1 = len(dataset) // 3\n",
    "# split_index_2 = 2 * len(dataset) // 3\n",
    "# # Split the dataset into three parts\n",
    "# first_third = dataset[:split_index_1]\n",
    "# second_third = dataset[split_index_1:split_index_2]\n",
    "# third_third = dataset[split_index_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 405/405 [03:08<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float16\n",
      "tensor saved in: f16val_tensor_data.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 0.3333,  0.3491,  0.3726,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.3254,  0.3411,  0.3647,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 0.3411,  0.3491,  0.3726,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           ...,\n",
       "           [-0.7803, -0.7803, -0.7803,  ...,  0.9136,  0.8979,  0.8667],\n",
       "           [-0.7881, -0.7881, -0.7803,  ...,  0.9058,  0.8823,  0.8667],\n",
       "           [-0.7881, -0.7881, -0.7881,  ...,  0.8979,  0.8745,  0.8511]],\n",
       "\n",
       "          [[ 0.0196,  0.0353,  0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           [ 0.0118,  0.0275,  0.0667,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           [ 0.0275,  0.0353,  0.0588,  ...,  1.0000,  1.0000,  1.0000],\n",
       "           ...,\n",
       "           [-0.7256, -0.7256, -0.7334,  ...,  0.2627,  0.2471,  0.2313],\n",
       "           [-0.7334, -0.7334, -0.7334,  ...,  0.2549,  0.2313,  0.2157],\n",
       "           [-0.7334, -0.7334, -0.7412,  ...,  0.2313,  0.2235,  0.2000]],\n",
       "\n",
       "          [[ 0.0275,  0.0196,  0.0039,  ...,  0.9609,  0.9609,  0.9609],\n",
       "           [ 0.0196,  0.0118, -0.0039,  ...,  0.9609,  0.9609,  0.9609],\n",
       "           [ 0.0275,  0.0196, -0.0039,  ...,  0.9609,  0.9609,  0.9609],\n",
       "           ...,\n",
       "           [-0.6470, -0.6470, -0.6392,  ...,  0.2313,  0.2157,  0.2000],\n",
       "           [-0.6548, -0.6548, -0.6392,  ...,  0.2235,  0.2000,  0.1843],\n",
       "           [-0.6548, -0.6548, -0.6470,  ...,  0.2079,  0.1921,  0.1686]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.5767,  0.8589,  0.3254,  ..., -0.0902,  0.0118,  0.3647],\n",
       "           [ 0.8354,  0.9453,  0.0902,  ...,  0.3254,  0.4119,  0.4119],\n",
       "           [ 0.2393,  0.5137, -0.1686,  ...,  0.3882,  0.4353,  0.2549],\n",
       "           ...,\n",
       "           [ 0.2471,  0.1843,  0.2627,  ...,  0.2313,  0.3254,  0.3411],\n",
       "           [ 0.2157,  0.2235,  0.3411,  ...,  0.3569,  0.3569,  0.1765],\n",
       "           [ 0.4038,  0.3333,  0.2783,  ...,  0.2000,  0.2705,  0.1530]],\n",
       "\n",
       "          [[ 0.6157,  0.8901,  0.2864,  ..., -0.1059, -0.0275,  0.3254],\n",
       "           [ 0.8745,  0.9688,  0.0823,  ...,  0.3254,  0.3960,  0.3960],\n",
       "           [ 0.2942,  0.5688, -0.1451,  ...,  0.4353,  0.4824,  0.2864],\n",
       "           ...,\n",
       "           [ 0.1530,  0.0902,  0.1686,  ...,  0.1530,  0.2471,  0.2627],\n",
       "           [ 0.1216,  0.1294,  0.2471,  ...,  0.2783,  0.2783,  0.0980],\n",
       "           [ 0.3098,  0.2393,  0.1843,  ...,  0.1216,  0.1921,  0.0745]],\n",
       "\n",
       "          [[ 0.6392,  0.9136,  0.3411,  ..., -0.4275, -0.3411, -0.0039],\n",
       "           [ 0.9058,  1.0000,  0.1294,  ...,  0.0275,  0.0902,  0.0902],\n",
       "           [ 0.3411,  0.6157, -0.0902,  ...,  0.1686,  0.2157,  0.0275],\n",
       "           ...,\n",
       "           [ 0.1843,  0.1216,  0.2000,  ...,  0.1608,  0.2549,  0.2705],\n",
       "           [ 0.1530,  0.1608,  0.2783,  ...,  0.2864,  0.2864,  0.1059],\n",
       "           [ 0.3411,  0.2705,  0.2157,  ...,  0.1294,  0.2000,  0.0823]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-0.6392, -0.5845, -0.5845,  ...,  0.2079,  0.2235,  0.2079],\n",
       "           [-0.8589, -0.6782, -0.5688,  ...,  0.2393,  0.2471,  0.2235],\n",
       "           [-0.7334, -0.8354, -0.8037,  ...,  0.2705,  0.2783,  0.2627],\n",
       "           ...,\n",
       "           [-0.1294, -0.0667, -0.0039,  ...,  0.1686,  0.2549,  0.3491],\n",
       "           [-0.1216, -0.0667, -0.0118,  ...,  0.1686,  0.1059,  0.0823],\n",
       "           [-0.1059, -0.0745, -0.0432,  ...,  0.1608,  0.1451,  0.1530]],\n",
       "\n",
       "          [[-0.7568, -0.7021, -0.6943,  ...,  0.0275,  0.0196,  0.0039],\n",
       "           [-0.9766, -0.7959, -0.6782,  ...,  0.0432,  0.0432,  0.0196],\n",
       "           [-0.8433, -0.9453, -0.9136,  ...,  0.0745,  0.0745,  0.0353],\n",
       "           ...,\n",
       "           [-0.2627, -0.2000, -0.1372,  ...,  0.0353,  0.1216,  0.2157],\n",
       "           [-0.2549, -0.2000, -0.1451,  ...,  0.0353, -0.0275, -0.0510],\n",
       "           [-0.2393, -0.2079, -0.1765,  ...,  0.0275,  0.0118,  0.0196]],\n",
       "\n",
       "          [[-0.8115, -0.7568, -0.7646,  ..., -0.2157, -0.2393, -0.2705],\n",
       "           [-1.0000, -0.8511, -0.7490,  ..., -0.1921, -0.2157, -0.2549],\n",
       "           [-0.9136, -1.0000, -0.9844,  ..., -0.1608, -0.1843, -0.2313],\n",
       "           ...,\n",
       "           [-0.3411, -0.2783, -0.2157,  ..., -0.0275,  0.0588,  0.1530],\n",
       "           [-0.3333, -0.2783, -0.2235,  ..., -0.0275, -0.0902, -0.1137],\n",
       "           [-0.3176, -0.2864, -0.2549,  ..., -0.0353, -0.0510, -0.0432]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-1.0000, -0.9922, -0.9688,  ..., -1.0000, -1.0000, -0.9766],\n",
       "           [-1.0000, -0.9922, -0.9688,  ..., -1.0000, -0.9609, -0.9058],\n",
       "           [-1.0000, -0.9922, -0.9688,  ..., -0.9292, -0.8979, -0.8589],\n",
       "           ...,\n",
       "           [-1.0000, -0.9922, -0.9766,  ..., -0.9688, -0.9688, -0.9688],\n",
       "           [-1.0000, -0.9844, -0.9766,  ..., -0.9766, -0.9688, -0.9688],\n",
       "           [-1.0000, -0.9844, -0.9766,  ..., -0.9766, -0.9688, -0.9688]],\n",
       "\n",
       "          [[-0.9844, -0.9922, -1.0000,  ..., -0.3960, -0.2549, -0.1530],\n",
       "           [-0.9844, -0.9922, -1.0000,  ..., -0.2313, -0.1059, -0.0196],\n",
       "           [-0.9844, -0.9922, -1.0000,  ..., -0.0275,  0.0745,  0.1372],\n",
       "           ...,\n",
       "           [-0.9922, -0.9844, -0.9844,  ...,  0.2549,  0.2705,  0.2705],\n",
       "           [-0.9922, -0.9766, -0.9844,  ...,  0.2471,  0.2549,  0.2549],\n",
       "           [-0.9922, -0.9766, -0.9844,  ...,  0.2471,  0.2549,  0.2549]],\n",
       "\n",
       "          [[-1.0000, -0.9922, -0.9058,  ..., -0.4038, -0.2705, -0.1843],\n",
       "           [-1.0000, -0.9922, -0.9058,  ..., -0.2627, -0.1451, -0.0667],\n",
       "           [-1.0000, -0.9922, -0.9058,  ..., -0.0823, -0.0039,  0.0510],\n",
       "           ...,\n",
       "           [-0.9766, -0.9531, -0.9370,  ...,  0.1451,  0.1530,  0.1530],\n",
       "           [-0.9609, -0.9453, -0.9370,  ...,  0.1372,  0.1451,  0.1451],\n",
       "           [-0.9609, -0.9453, -0.9214,  ...,  0.1372,  0.1451,  0.1451]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-0.2157, -0.2157, -0.2157,  ..., -0.7881, -0.8511, -0.7021],\n",
       "           [-0.2157, -0.2157, -0.2079,  ..., -0.5137, -0.7412, -0.8823],\n",
       "           [-0.2157, -0.2079, -0.2079,  ..., -0.3176, -0.5527, -0.8433],\n",
       "           ...,\n",
       "           [-0.8198, -0.8198, -0.8198,  ..., -0.0902, -0.1843, -0.2079],\n",
       "           [-0.8198, -0.8198, -0.8198,  ..., -0.0667, -0.1451, -0.2079],\n",
       "           [-0.8198, -0.8198, -0.8198,  ..., -0.1294, -0.1765, -0.2393]],\n",
       "\n",
       "          [[-0.4353, -0.4353, -0.4353,  ..., -0.9370, -1.0000, -0.8511],\n",
       "           [-0.4353, -0.4353, -0.4275,  ..., -0.6626, -0.8901, -1.0000],\n",
       "           [-0.4353, -0.4275, -0.4275,  ..., -0.4666, -0.7021, -0.9922],\n",
       "           ...,\n",
       "           [-0.9292, -0.9292, -0.9292,  ..., -0.5137, -0.6235, -0.6548],\n",
       "           [-0.9292, -0.9292, -0.9292,  ..., -0.4902, -0.5845, -0.6470],\n",
       "           [-0.9292, -0.9292, -0.9292,  ..., -0.5527, -0.6157, -0.6782]],\n",
       "\n",
       "          [[-0.5215, -0.5215, -0.5215,  ..., -1.0000, -1.0000, -0.9609],\n",
       "           [-0.5215, -0.5215, -0.5137,  ..., -0.7725, -1.0000, -1.0000],\n",
       "           [-0.5215, -0.5137, -0.5137,  ..., -0.5767, -0.8115, -1.0000],\n",
       "           ...,\n",
       "           [-0.9370, -0.9370, -0.9370,  ..., -0.7334, -0.8354, -0.8667],\n",
       "           [-0.9370, -0.9370, -0.9370,  ..., -0.7100, -0.7959, -0.8589],\n",
       "           [-0.9370, -0.9370, -0.9370,  ..., -0.7725, -0.8276, -0.8901]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-0.1843, -0.1765, -0.1137,  ..., -0.2000, -0.1451, -0.1372],\n",
       "           [-0.0588, -0.0432, -0.0902,  ..., -0.0353, -0.0353, -0.1372],\n",
       "           [-0.2471, -0.1372, -0.1451,  ...,  0.0667,  0.0667, -0.1451],\n",
       "           ...,\n",
       "           [-0.0902,  0.2235,  0.1451,  ...,  0.3254,  0.1451, -0.0823],\n",
       "           [ 0.0118,  0.2157,  0.2079,  ...,  0.2079,  0.0745, -0.0667],\n",
       "           [-0.2000, -0.0980, -0.1059,  ..., -0.0510, -0.1294, -0.1765]],\n",
       "\n",
       "          [[-0.5137, -0.5059, -0.4275,  ..., -0.4038, -0.4197, -0.4197],\n",
       "           [-0.3647, -0.3333, -0.3882,  ..., -0.2393, -0.2864, -0.4119],\n",
       "           [-0.5059, -0.3882, -0.3960,  ..., -0.1294, -0.1530, -0.3960],\n",
       "           ...,\n",
       "           [-0.3491, -0.0353, -0.1137,  ...,  0.1059, -0.0902, -0.3176],\n",
       "           [-0.2705, -0.0667, -0.0745,  ..., -0.0118, -0.1608, -0.3020],\n",
       "           [-0.4824, -0.3804, -0.3960,  ..., -0.2705, -0.3726, -0.4197]],\n",
       "\n",
       "          [[-0.6235, -0.6157, -0.5215,  ..., -0.5845, -0.5923, -0.5923],\n",
       "           [-0.4824, -0.4587, -0.4902,  ..., -0.4197, -0.4509, -0.5845],\n",
       "           [-0.6235, -0.5059, -0.4980,  ..., -0.3020, -0.3176, -0.5605],\n",
       "           ...,\n",
       "           [-0.8901, -0.5767, -0.6392,  ..., -0.4746, -0.6548, -0.8823],\n",
       "           [-0.7568, -0.5527, -0.5449,  ..., -0.5845, -0.7100, -0.8511],\n",
       "           [-0.9531, -0.8354, -0.8276,  ..., -0.8433, -0.8979, -0.9453]]]]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature_extraction_fn(val_dataset[\"image_path\"], tensor_name=\"f16val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extraction_fn(second_third[\"image_path\"], tensor_name=\"train2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extraction_fn(third_third[\"image_path\"], tensor_name=\"train3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import concurrent.futures\n",
    "\n",
    "# # Define your function to process a batch of tensors\n",
    "# def process_tensor_batch(batch_tensor):\n",
    "#     # Perform your processing operations here\n",
    "#     # Example: Multiply each element in the tensor by 2\n",
    "#     processed_batch = batch_tensor * 1\n",
    "#     return processed_batch\n",
    "\n",
    "# # Load your tensor (example)\n",
    "# tensor = torch.load('train1_tensor_data.pt')\n",
    "\n",
    "# # Split the tensor into batches (adjust batch_size based on your needs)\n",
    "# batch_size = 100\n",
    "# max_workers = 10\n",
    "# tensor_batches = [tensor[i:i + batch_size] for i in range(0, len(tensor), batch_size)]\n",
    "\n",
    "# # Define a function to process a single batch in parallel\n",
    "# def process_batch_parallel(batch):\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # Process the batch in parallel using ThreadPoolExecutor\n",
    "#         processed_batch_list = list(executor.map(process_tensor_batch, batch))\n",
    "#     return processed_batch_list\n",
    "\n",
    "# # Process tensor batches in parallel\n",
    "# processed_batches = process_batch_parallel(tensor_batches)\n",
    "\n",
    "# # Concatenate the processed batches back into a single tensor\n",
    "# processed_tensor = torch.cat([torch.tensor(batch) for batch in processed_batches], dim=0)\n",
    "\n",
    "# # Now, 'processed_tensor' contains the processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tensor1 = torch.load('f16train1_tensor_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27537, 1, 3, 224, 224])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tensor1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tensor2 = torch.load('f16train2_tensor_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tensor3 = torch.load('f16train3_tensor_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'dataset' is your dataset object\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('train_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'dataset' is your dataset object\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(val_dataset)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('val_dataset.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# text preprocessing step\n",
    "#gpt version\n",
    "def tokenization_fn(captions, max_target_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    tokenized_labels = []\n",
    "    # Initialize tqdm progress bar with the total number of captions\n",
    "    with tqdm(total=len(captions), desc=\"Tokenizing Captions\") as pbar:\n",
    "        # Iterate through captions and tokenize them\n",
    "        for caption in captions:\n",
    "            # Tokenize the caption using the tokenizer\n",
    "            tokens = tokenizer(caption, padding=\"max_length\", max_length=max_target_length).input_ids\n",
    "            # Append tokenized caption to the list\n",
    "            tokenized_labels.append(tokens)\n",
    "            pbar.update(1)  # Update the progress bar\n",
    "    # Convert the list of tokenized labels to a PyTorch tensor\n",
    "    labels = torch.tensor(tokenized_labels, dtype=torch.long)\n",
    "    pbar.close()\n",
    "\n",
    "    return labels\n",
    "\n",
    "#original version\n",
    "# def tokenization_fn(captions, max_target_length):\n",
    "#     \"\"\"Run tokenization on captions.\"\"\"\n",
    "#     labels = tokenizer(captions,\n",
    "#                       padding=\"max_length\",\n",
    "#                       max_length=max_target_length).input_ids\n",
    "\n",
    "#     return labels\n",
    "\n",
    "\n",
    "# image preprocessing step\n",
    "#original version\n",
    "# def feature_extraction_fn(image_paths, check_image=True):\n",
    "#     \"\"\"\n",
    "#     Run feature extraction on images\n",
    "#     If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n",
    "#     Otherwise, an exception will be thrown.\n",
    "#     \"\"\"\n",
    "#     inputs = []\n",
    "#     counter=0\n",
    "\n",
    "#     if check_image==False:\n",
    "#         images = []\n",
    "#         to_keep = []\n",
    "#         for image_file in image_paths:\n",
    "#             try:\n",
    "#                 img = Image.open(image_file)\n",
    "#                 #if img.mode != 'RGB':\n",
    "#                    #img = img.convert('RGB')\n",
    "#                 images.append(img)\n",
    "#                 to_keep.append(True)               \n",
    "#             except Exception:\n",
    "#                 to_keep.append(False)\n",
    "#     else:\n",
    "#         #images = [Image.open(image_file) for image_file in image_paths]\n",
    "#     #     for image_file in image_paths:\n",
    "#     #         with Image.open(image_file) as test_image:\n",
    "#     #             image_paths[counter] = feature_extractor(images=test_image, return_tensors=\"np\")\n",
    "#     #             counter+=1\n",
    "#     # encoder_inputs = feature_extractor(images=image_paths, return_tensors=\"np\")\n",
    "#     # return encoder_inputs.pixel_values\n",
    "#         with tqdm(total=len(image_paths), desc=\"Processing Images\") as pbar:  \n",
    "#             for image_file in image_paths:\n",
    "#                 #with Image.open(image_file) as test_image:\n",
    "#                 img = Image.open(image_file)\n",
    "#                 image_path = feature_extractor(img, return_tensors=\"np\")\n",
    "#                 inputs.append(image_path.pixel_values)\n",
    "#                 img.close()\n",
    "#                 pbar.update(1) \n",
    "#     numpy_array = np.array(inputs)\n",
    "#     tensor = torch.tensor(numpy_array)\n",
    "#     #tensor = torch.tensor(inputs, dtype=torch.float32)            \n",
    "#     return tensor            \n",
    "\n",
    "#     #return encoder_inputs.pixel_values\n",
    "\n",
    "def preprocess_fn(examples, max_target_length, tensor):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    #image_paths = examples['image_path']\n",
    "    captions = examples['caption']\n",
    "    model_inputs = {}\n",
    "    # This contains image path column\n",
    "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
    "    #model_inputs['pixel_values'] = feature_extraction_fn(image_paths)\n",
    "    model_inputs['pixel_values'] = tensor#torch.load(tensor)\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = tokenization_fn(val_dataset[\"caption\"], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_fn(examples=val_dataset, max_target_length=100, tensor=loaded_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.cat((loaded_tensor1, loaded_tensor2, loaded_tensor3), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Validation Dataset:   0%|          | 0/82611 [00:00<?, ?it/s]Parameter 'function'=<function process_example at 0x7fbd337e7520> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "\n",
      "\u001b[A\n",
      "Tokenizing Captions: 100%|██████████| 1000/1000 [00:00<00:00, 6542.85it/s]\n",
      "Processing Validation Dataset:   0%|          | 1/82611 [00:09<215:52:33,  9.41s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m processed_example\n\u001b[1;32m     10\u001b[0m \u001b[39m# Apply the preprocess_fn to the validation dataset using map\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m processed_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     12\u001b[0m     function\u001b[39m=\u001b[39;49mprocess_example,\n\u001b[1;32m     13\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m     \u001b[39m#remove_columns=val_dataset.column_names,\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[39m# Close the progress bar after processing is complete\u001b[39;00m\n\u001b[1;32m     18\u001b[0m progress_bar[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3087\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3081\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3082\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3085\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3087\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3088\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3089\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/arrow_dataset.py:3480\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(batch)\n\u001b[1;32m   3479\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3480\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[1;32m   3481\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_examples_in_batch\n\u001b[1;32m   3482\u001b[0m \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m _time \u001b[39m+\u001b[39m config\u001b[39m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/arrow_writer.py:553\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    551\u001b[0m         col_try_type \u001b[39m=\u001b[39m try_features[col] \u001b[39mif\u001b[39;00m try_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m col \u001b[39min\u001b[39;00m try_features \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m         typed_sequence \u001b[39m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mcol_type, try_type\u001b[39m=\u001b[39mcol_try_type, col\u001b[39m=\u001b[39mcol)\n\u001b[0;32m--> 553\u001b[0m         arrays\u001b[39m.\u001b[39mappend(pa\u001b[39m.\u001b[39;49marray(typed_sequence))\n\u001b[1;32m    554\u001b[0m         inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[1;32m    555\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/pyarrow/array.pxi:243\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/pyarrow/array.pxi:110\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/arrow_writer.py:189\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     trying_cast_to_python_objects \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     out \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39marray(cast_to_python_objects(data, only_1d_for_numpy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    190\u001b[0m \u001b[39m# use smaller integer precisions if possible\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrying_int_optimization:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:446\u001b[0m, in \u001b[0;36mcast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcast_to_python_objects\u001b[39m(obj: Any, only_1d_for_numpy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, optimize_list_casting\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    427\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    Cast numpy/pytorch/tensorflow/pandas objects to python lists.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m    It works recursively.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39m        casted_obj: the casted object\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m _cast_to_python_objects(\n\u001b[1;32m    447\u001b[0m         obj, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    448\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:319\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 319\u001b[0m             [\n\u001b[1;32m    320\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m    321\u001b[0m                     x, only_1d_for_numpy\u001b[39m=\u001b[39monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39moptimize_list_casting\n\u001b[1;32m    322\u001b[0m                 )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    323\u001b[0m                 \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    324\u001b[0m             ],\n\u001b[1;32m    325\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m         )\n\u001b[1;32m    327\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mTF_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:320\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    319\u001b[0m             [\n\u001b[0;32m--> 320\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m    321\u001b[0m                     x, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    322\u001b[0m                 )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    323\u001b[0m                 \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    324\u001b[0m             ],\n\u001b[1;32m    325\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m         )\n\u001b[1;32m    327\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mTF_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:304\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[39mreturn\u001b[39;00m obj, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 304\u001b[0m             [\n\u001b[1;32m    305\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m    306\u001b[0m                     x, only_1d_for_numpy\u001b[39m=\u001b[39monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39moptimize_list_casting\n\u001b[1;32m    307\u001b[0m                 )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    308\u001b[0m                 \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m obj\n\u001b[1;32m    309\u001b[0m             ],\n\u001b[1;32m    310\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mTORCH_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:305\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[39mreturn\u001b[39;00m obj, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    304\u001b[0m             [\n\u001b[0;32m--> 305\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m    306\u001b[0m                     x, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    307\u001b[0m                 )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    308\u001b[0m                 \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m obj\n\u001b[1;32m    309\u001b[0m             ],\n\u001b[1;32m    310\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mTORCH_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:304\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[39mreturn\u001b[39;00m obj, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 304\u001b[0m             [\n\u001b[1;32m    305\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m    306\u001b[0m                     x, only_1d_for_numpy\u001b[39m=\u001b[39monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39moptimize_list_casting\n\u001b[1;32m    307\u001b[0m                 )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    308\u001b[0m                 \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m obj\n\u001b[1;32m    309\u001b[0m             ],\n\u001b[1;32m    310\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mTORCH_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:305\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[39mreturn\u001b[39;00m obj, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    304\u001b[0m             [\n\u001b[0;32m--> 305\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m    306\u001b[0m                     x, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    307\u001b[0m                 )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    308\u001b[0m                 \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m obj\n\u001b[1;32m    309\u001b[0m             ],\n\u001b[1;32m    310\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mTORCH_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:304\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[39mreturn\u001b[39;00m obj, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 304\u001b[0m             [\n\u001b[1;32m    305\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m    306\u001b[0m                     x, only_1d_for_numpy\u001b[39m=\u001b[39monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39moptimize_list_casting\n\u001b[1;32m    307\u001b[0m                 )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    308\u001b[0m                 \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m obj\n\u001b[1;32m    309\u001b[0m             ],\n\u001b[1;32m    310\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mTORCH_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:305\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[39mreturn\u001b[39;00m obj, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    304\u001b[0m             [\n\u001b[0;32m--> 305\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m    306\u001b[0m                     x, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    307\u001b[0m                 )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    308\u001b[0m                 \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m obj\n\u001b[1;32m    309\u001b[0m             ],\n\u001b[1;32m    310\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mTORCH_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/datasets/features/features.py:297\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mPIL_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mPIL\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[1;32m    295\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mImage\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(obj, np\u001b[39m.\u001b[39;49mndarray):\n\u001b[1;32m    298\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    299\u001b[0m         \u001b[39mreturn\u001b[39;00m obj[()], \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###gpt version\n",
    "progress_bar = [tqdm(total=len(dataset), desc=\"Processing Validation Dataset\")]\n",
    "\n",
    "# Define a function to preprocess a single example and update the progress bar\n",
    "def process_example(example):\n",
    "    processed_example = preprocess_fn(example, max_target_length=200, tensor=tensor)\n",
    "    progress_bar[0].update(1)  # Update the progress bar after processing each example\n",
    "    return processed_example\n",
    "\n",
    "# Apply the preprocess_fn to the validation dataset using map\n",
    "processed_dataset = dataset.map(\n",
    "    function=process_example,\n",
    "    batched=True,\n",
    "    #remove_columns=val_dataset.column_names,\n",
    ")\n",
    "\n",
    "# Close the progress bar after processing is complete\n",
    "progress_bar[0].close()\n",
    "###original version***    \n",
    "# val_processed_dataset = val_dataset.map(\n",
    "#     function=preprocess_fn(val_dataset, max_target_length=200, tensor=loaded_tensor),\n",
    "#     batched=True,\n",
    "#     fn_kwargs={\"max_target_length\": 200},\n",
    "#     #remove_columns=dataset.column_names\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_fn(dataset, 15, check_image = True)\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 200},\n",
    "    #remove_columns=val_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    output_dir=\"./image-captioning-output-107epochs\",\n",
    "    num_train_epochs= 20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=val_processed_dataset,\n",
    "    #eval_dataset=val_updated_dataset,\n",
    "    eval_dataset=val_processed_dataset,\n",
    "    #train_dataset=processed_dataset['train'],\n",
    "    #eval_dataset=processed_dataset['validation'],\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.compute_metrics\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./image-captioning-output-107epochs\")\n",
    "\n",
    "tokenizer.save_pretrained(\"./image-captioning-output-107epochs\")\n",
    "\n",
    "from transformers import pipeline\n",
    "image_captioner = pipeline(\"image-to-text\", model=\"./image-captioning-output-107epochs\", max_new_tokens=10)\n",
    "dataset[\"image\"][5]\n",
    "image_captioner(dataset['image'][5])\n",
    "image_captioner(\"test_images/COCO_test2015_000000000014.jpg\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
