{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 The person1 is to the right of the tie']\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"/data1/ViTgpt2/image-captioning-output\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"/data1/ViTgpt2/image-captioning-output\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data1/ViTgpt2/image-captioning-output\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values.to(device)\n",
    "\n",
    "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds\n",
    "\n",
    "\n",
    "print(predict_step(['/home/vcl3d/coco_dataset_VOX/test2015/COCO_test2015_000000000202.jpg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (q_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using framework PyTorch: 2.0.1+cu118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 2.0.1+cu118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 2.0.1+cu118\n",
      "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['onnx/preprocessor_config.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForImageClassification\n",
    "from optimum.onnxruntime import ORTModelForVision2Seq\n",
    "from transformers import AutoFeatureExtractor\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_id=\"/data1/ViTgpt2/image-captioning-output\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForVision2Seq.from_pretrained(model_id, from_transformers=False)\n",
    "preprocessor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "preprocessor.save_pretrained(onnx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '2 The stop sign is to the left of the train'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vanilla_clf = pipeline(\"image-to-text\", model=model, feature_extractor=preprocessor, tokenizer=tokenizer)\n",
    "print(vanilla_clf(\"https://datasets-server.huggingface.co/assets/visual_genome/--/attributes_v1.0.0/train/0/image/image.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['onnx/preprocessor_config.json']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from optimum.onnxruntime import ORTModelForImageClassification\n",
    "# from optimum.onnxruntime import ORTModelForVision2Seq\n",
    "# from transformers import AutoFeatureExtractor, VisionEncoderDecoderConfig\n",
    "# from pathlib import Path\n",
    "\n",
    "# from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel, GPT2Config\n",
    "# model_id=\"/data1/ViTgpt2/image-captioning-output\"\n",
    "# onnx_path = Path(\"onnx\")\n",
    "# # Initializing a ViT & BERT style configuration\n",
    "# config_encoder = ViTConfig()\n",
    "# #config_decoder = BertConfig()\n",
    "# config_decoder = GPT2Config()\n",
    "\n",
    "# config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "\n",
    "# # Initializing a ViTBert model (with random weights) from a ViT & bert-base-uncased style configurations\n",
    "# model = VisionEncoderDecoderModel(config=config)\n",
    "\n",
    "\n",
    "# config_encoder = model.config.encoder\n",
    "# config_decoder = model.config.decoder\n",
    "\n",
    "# config_decoder.is_decoder = True\n",
    "# config_decoder.add_cross_attention = True\n",
    "\n",
    "# model.save_pretrained(model_id)\n",
    "\n",
    "# encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained(model_id)\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(model_id)\n",
    "# # load vanilla transformers and convert to onnx\n",
    "# #model = ORTModelForVision2Seq.from_pretrained(model_id, from_transformers=True)\n",
    "# preprocessor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# # save onnx checkpoint and tokenizer\n",
    "# model.save_pretrained(onnx_path)\n",
    "# preprocessor.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Names: ['pixel_values']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import onnx\n",
    "decoder_model_path = '/data1/ViTgpt2/onnx/decoder_model.onnx'\n",
    "decoder_onnx_model = onnx.load(decoder_model_path)\n",
    "encoder_model_path = '/data1/ViTgpt2/onnx/encoder_model.onnx'\n",
    "encoder_onnx_model = onnx.load(encoder_model_path)\n",
    "decoder_session = ort.InferenceSession(decoder_model_path)\n",
    "encoder_session = ort.InferenceSession(encoder_model_path)\n",
    "input_names = [input.name for input in encoder_onnx_model.graph.input]\n",
    "print(\"Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Names: ['input_ids', 'encoder_hidden_states']\n"
     ]
    }
   ],
   "source": [
    "output_names = [input.name for input in decoder_onnx_model.graph.input]\n",
    "print(\"Output Names:\", output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_ids'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPUExecutionProvider']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_session.get_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names\n",
    "#inputs = {'input_ids': input_names[0], 'encoder_hidden_states': input_names[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pairs = {}\n",
    "\n",
    "# Iterate through the list of strings\n",
    "for string in input_names:\n",
    "    # Split the string into key and value based on the last dot (.)\n",
    "    parts = string.split('.')\n",
    "    if len(parts) == 2:\n",
    "        key, value = parts\n",
    "    else:\n",
    "        key = string\n",
    "        value = string\n",
    "    \n",
    "    # Add the key-value pair to the dictionary\n",
    "    key_value_pairs[key] = value\n",
    "\n",
    "# Now, key_value_pairs contains the key-value pairs\n",
    "print(key_value_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_pairs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load and preprocess an image\n",
    "input_image = Image.open('/home/vcl3d/coco_dataset_VOX_mini/train2014/COCO_train2014_000000000009.jpg')\n",
    "input_image = input_image.resize((224, 224))  # Resize to match model input size\n",
    "input_image = np.array(input_image)  # Convert to numpy array\n",
    "input_image = (input_image / 255.0).astype(np.float32)  # Normalize pixel values\n",
    "#input_image = np.transpose(input_image, (2, 0, 1))  # Change data layout if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "encoder_session = InferenceSession('/data1/ViTgpt2/onnx/encoder_model.onnx')\n",
    "decoder_session = InferenceSession('/data1/ViTgpt2/onnx/decoder_model.onnx')\n",
    "#ONNX Runtime expects NumPy arrays as input\n",
    "#outputs = session.run(output_names, input_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39m/home/vcl3d/coco_dataset_VOX_mini/train2014/COCO_train2014_000000000009.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m pixel_values \u001b[39m=\u001b[39m feature_extractor(images\u001b[39m=\u001b[39mimage, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mpixel_values\n\u001b[0;32m----> 3\u001b[0m gen_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpixel_values)\n",
      "\u001b[0;31mTypeError\u001b[0m: transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": [
    "image = Image.open('/home/vcl3d/coco_dataset_VOX_mini/train2014/COCO_train2014_000000000009.jpg')\n",
    "pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "gen_tokens = model.generate(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/ViTgpt2/gpt2env/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[50256,    17,   383,  9396,    16,   318,  2029,   262,  9396,    18,\n",
       "         50256]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTImageProcessor, VisionEncoderDecoderConfig\n",
    "processor = ViTImageProcessor.from_pretrained(\"Centaur31/myVitGpt2\")\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "gen_tokens = model.generate(**inputs)\n",
    "gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 The bowl1 is above the bowl3']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel, GPT2Config\n",
    "\n",
    "# Initializing a ViT & BERT style configuration\n",
    "config_encoder = ViTConfig()\n",
    "config_decoder = BertConfig()\n",
    "\n",
    "config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "\n",
    "# Initializing a ViTBert model (with random weights) from a ViT & bert-base-uncased style configurations\n",
    "model = VisionEncoderDecoderModel(config=config)\n",
    "\n",
    "# Accessing the model configuration\n",
    "config_encoder = model.config.encoder\n",
    "config_decoder = model.config.decoder\n",
    "# set decoder config to causal lm\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True\n",
    "\n",
    "# Saving the model, including its configuration\n",
    "model.save_pretrained(\"/data1/ViTgpt2/image-captioning-output\")\n",
    "\n",
    "# loading model and config from pretrained folder\n",
    "encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained(\"/data1/ViTgpt2/image-captioning-output\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"/data1/ViTgpt2/image-captioning-output\", config=encoder_decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoTokenizer, ViTImageProcessor\n",
    "from optimum.onnxruntime import ORTModelForVision2Seq\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\"Centaur31/myVitGpt2\")\n",
    "encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained(\"/data1/ViTgpt2/image-captioning-output\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Centaur31/myVitGpt2\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"/data1/ViTgpt2/image-captioning-output\", config=encoder_decoder_config)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "gen_tokens = model.generate(**inputs)\n",
    "outputs = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForVision2Seq\n",
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\"Centaur31/myVitGpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Centaur31/myVitGpt2\")\n",
    "model = ORTModelForSeq2SeqLM.from_pretrained(\"Centaur31/myVitGpt2\", export=True)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "gen_tokens = model.generate(**inputs)\n",
    "outputs = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "# Initializing a GPT2 configuration\n",
    "configuration = GPT2Config()\n",
    "\n",
    "# Initializing a model (with random weights) from the configuration\n",
    "model = GPT2Model(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
